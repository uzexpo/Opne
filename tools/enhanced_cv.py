#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ Enhanced Computer Vision for Open Interpreter
–ë—ã—Å—Ç—Ä–æ–µ –∏ —Ç–æ—á–Ω–æ–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –±–µ–∑ —Ç—è–∂–µ–ª—ã—Ö –º–æ–¥–µ–ª–µ–π
"""

import io
import time
import base64
import json
import mss
import numpy as np
from PIL import Image
from typing import List, Tuple, Dict, Optional
import logging

logger = logging.getLogger(__name__)

class SmartScreenCapture:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞—Ö–≤–∞—Ç —ç–∫—Ä–∞–Ω–∞"""
    
    def __init__(self):
        self.sct = mss.mss()
        self.cache = {}
        self.last_capture_time = 0
        self.min_interval = 0.1  # –ú–∏–Ω–∏–º—É–º 100ms –º–µ–∂–¥—É –∑–∞—Ö–≤–∞—Ç–∞–º–∏
    
    def capture_screen(self, region: Optional[Tuple[int, int, int, int]] = None) -> np.ndarray:
        """
        –ë—ã—Å—Ç—Ä—ã–π –∑–∞—Ö–≤–∞—Ç —ç–∫—Ä–∞–Ω–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        region: (x, y, width, height) –∏–ª–∏ None –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Ä–∞–Ω–∞
        """
        current_time = time.time()
        
        # Throttling –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è spam –∑–∞—Ö–≤–∞—Ç–æ–≤
        if current_time - self.last_capture_time < self.min_interval:
            if 'last_frame' in self.cache:
                return self.cache['last_frame']
        
        try:
            if region:
                monitor = {"top": region[1], "left": region[0], 
                          "width": region[2], "height": region[3]}
            else:
                monitor = self.sct.monitors[1]  # –ü–µ—Ä–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä
            
            # MSS –∑–∞—Ö–≤–∞—Ç - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π!
            screenshot = self.sct.grab(monitor)
            img_array = np.array(screenshot)[:, :, :3]  # –£–±–∏—Ä–∞–µ–º –∞–ª—å—Ñ–∞ –∫–∞–Ω–∞–ª
            
            self.cache['last_frame'] = img_array
            self.last_capture_time = current_time
            
            logger.info(f"üì∏ –ó–∞—Ö–≤–∞—Ç —ç–∫—Ä–∞–Ω–∞: {img_array.shape} –∑–∞ {(time.time()-current_time)*1000:.1f}ms")
            return img_array
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞ —ç–∫—Ä–∞–Ω–∞: {e}")
            return np.array([])

class LocalVisionProcessor:
    """–õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Ç—è–∂–µ–ª—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def find_ui_elements_basic(self, image: np.ndarray, target_text: str = None) -> List[Dict]:
        """
        –ë–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ UI —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Ü–≤–µ—Ç–æ–≤ –∏ –∫–æ–Ω—Ç—É—Ä–æ–≤
        """
        from PIL import Image, ImageEnhance
        
        results = []
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PIL
            pil_img = Image.fromarray(image)
            
            # –ü–æ–∏—Å–∫ –∫–Ω–æ–ø–∫–æ-–ø–æ–¥–æ–±–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
            button_regions = self._find_button_regions(pil_img)
            
            for i, region in enumerate(button_regions):
                results.append({
                    "type": "button_candidate",
                    "bbox": region,
                    "confidence": 0.7,  # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    "center": ((region[0] + region[2]) // 2, (region[1] + region[3]) // 2)
                })
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö UI —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return []
    
    def _find_button_regions(self, pil_img: Image.Image) -> List[Tuple[int, int, int, int]]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π (–∫–Ω–æ–ø–∫–∏, –ø–æ–ª—è)"""
        import cv2
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ OpenCV —Ñ–æ—Ä–º–∞—Ç
        opencv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2GRAY)
        
        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç—É—Ä–æ–≤
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        button_regions = []
        
        for contour in contours:
            # –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞
            x, y, w, h = cv2.boundingRect(contour)
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–∞–∑–º–µ—Ä—É (–≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∫–Ω–æ–ø–∫–∏)
            if 30 <= w <= 300 and 20 <= h <= 80:
                button_regions.append((x, y, x + w, y + h))
        
        return button_regions[:10]  # –¢–æ–ø 10 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

class CloudVisionAPI:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ–±–ª–∞—á–Ω—ã–º–∏ CV API (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key
        self.enabled = bool(api_key)
    
    def analyze_image_cloud(self, image: np.ndarray) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –æ–±–ª–∞—á–Ω–æ–µ API (–∑–∞–≥–ª—É—à–∫–∞)"""
        if not self.enabled:
            return []
        
        # TODO: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Google Vision API / Azure CV / AWS Rekognition
        logger.info("üåê –û–±–ª–∞—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π")
        return []

class EnhancedComputerVision:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"""
    
    def __init__(self, use_cloud: bool = False, api_key: str = None):
        self.capture = SmartScreenCapture()
        self.local_processor = LocalVisionProcessor()
        self.cloud_api = CloudVisionAPI(api_key) if use_cloud else None
        
        logger.info("üöÄ Enhanced Computer Vision –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def find_and_click(self, target_description: str, 
                      region: Optional[Tuple[int, int, int, int]] = None,
                      click: bool = True) -> Dict:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: –Ω–∞–π—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç –∏ –∫–ª–∏–∫–Ω—É—Ç—å
        """
        start_time = time.time()
        
        try:
            # 1. –ó–∞—Ö–≤–∞—Ç —ç–∫—Ä–∞–Ω–∞
            image = self.capture.capture_screen(region)
            if image.size == 0:
                return {"success": False, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞—Ö–≤–∞—Ç–∏—Ç—å —ç–∫—Ä–∞–Ω"}
            
            # 2. –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            elements = self.local_processor.find_ui_elements_basic(image, target_description)
            
            if not elements:
                return {"success": False, "error": "–≠–ª–µ–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}
            
            # 3. –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            best_element = max(elements, key=lambda x: x['confidence'])
            
            # 4. –ö–ª–∏–∫ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
            if click:
                import pyautogui
                x, y = best_element['center']
                pyautogui.click(x, y)
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è UI —Ä–µ–∞–∫—Ü–∏–∏
                time.sleep(0.2)
            
            processing_time = (time.time() - start_time) * 1000
            
            result = {
                "success": True,
                "element": best_element,
                "processing_time_ms": processing_time,
                "elements_found": len(elements)
            }
            
            logger.info(f"‚úÖ –≠–ª–µ–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {processing_time:.1f}ms")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ find_and_click: {e}")
            return {"success": False, "error": str(e)}
    
    def smart_screenshot_analysis(self, save_path: str = None) -> Dict:
        """–£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —ç–∫—Ä–∞–Ω–∞"""
        
        try:
            image = self.capture.capture_screen()
            elements = self.local_processor.find_ui_elements_basic(image)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
            if save_path:
                self._save_annotated_screenshot(image, elements, save_path)
            
            return {
                "success": True,
                "elements_count": len(elements),
                "elements": elements,
                "screen_size": image.shape[:2] if image.size > 0 else None
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —ç–∫—Ä–∞–Ω–∞: {e}")
            return {"success": False, "error": str(e)}
    
    def _save_annotated_screenshot(self, image: np.ndarray, elements: List[Dict], path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏"""
        try:
            import cv2
            
            annotated = image.copy()
            
            for element in elements:
                x1, y1, x2, y2 = element['bbox']
                cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(annotated, f"{element['confidence']:.2f}", 
                           (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            
            cv2.imwrite(path, cv2.cvtColor(annotated, cv2.COLOR_RGB2BGR))
            logger.info(f"üíæ –ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {e}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –Ω–∞–≤—ã–∫–∞—Ö
enhanced_cv = EnhancedComputerVision()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Open Interpreter
def smart_click(target_description: str, region: str = None) -> str:
    """
    –£–º–Ω—ã–π –∫–ª–∏–∫ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é —ç–ª–µ–º–µ–Ω—Ç–∞
    """
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–≥–∏–æ–Ω–∞ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        parsed_region = None
        if region:
            # –§–æ—Ä–º–∞—Ç: "x,y,width,height"
            coords = list(map(int, region.split(',')))
            parsed_region = tuple(coords)
        
        result = enhanced_cv.find_and_click(target_description, parsed_region, click=True)
        
        if result['success']:
            return f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∫–ª–∏–∫–Ω—É–ª –ø–æ —ç–ª–µ–º–µ–Ω—Ç—É. –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_time_ms']:.1f}ms"
        else:
            return f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç: {result['error']}"
            
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ smart_click: {e}"

def analyze_screen() -> str:
    """
    –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —ç–∫—Ä–∞–Ω–∞
    """
    try:
        result = enhanced_cv.smart_screenshot_analysis()
        
        if result['success']:
            return f"üìä –ê–Ω–∞–ª–∏–∑ —ç–∫—Ä–∞–Ω–∞: –Ω–∞–π–¥–µ–Ω–æ {result['elements_count']} UI —ç–ª–µ–º–µ–Ω—Ç–æ–≤"
        else:
            return f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {result['error']}"
            
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ analyze_screen: {e}"

if __name__ == "__main__":
    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Enhanced Computer Vision...")
    
    # –¢–µ—Å—Ç –∑–∞—Ö–≤–∞—Ç–∞ —ç–∫—Ä–∞–Ω–∞
    cv = EnhancedComputerVision()
    result = cv.smart_screenshot_analysis("test_screenshot.png")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞: {result}")
